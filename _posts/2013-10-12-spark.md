---
layout: post
title: "Spark計算流程詳細分析"
description: ""
category: ""
tags: [spark,大数据,分布式]
---
{% include JB/setup %}

概述
===
对于一个分布式的计算系统来说，一般而言，都可以从两个方面来描述：
- 计算任务的管理维护（划分，分配，监控），简而言之，一个任务的管理调度模块
- 任务的计算执行模块

参考这个逻辑，Spark也可以从这两个方面来做比较深入的分析，去领悟RDD设计的巧妙，并且与Hadoop等其他分布式系统相关组件进行对照。

任务调度
===
Spark中的术语叫做Driver Program，它是一个单独的JVM应用，跑在独立的虚拟机进程上，通过main方法启动。以example源码中SparkPi为例

```
/** Computes an approximation to pi */
object SparkPi {
        def main(args: Array[String]) {
                if (args.length == 0) {
                        System.err.println("Usage: SparkPi <master> [<slices>]")
                                System.exit(1)
                }
                val spark = new SparkContext(args(0), "SparkPi",
                                System.getenv("SPARK_HOME"), Seq(System.getenv("SPARK_EXAMPLES_JAR")))
                        val slices = if (args.length > 1) args(1).toInt else 2
                        val n = 100000 * slices
                        val count = spark.parallelize(1 to n, slices).map { i =>
                                val x = random * 2 - 1
                                        val y = random * 2 - 1
                                        if (x*x + y*y < 1) 1 else 0
                        }.reduce(_ + _)
                println("Pi is roughly " + 4.0 * count / n)
                        System.exit(0)
        }
}
```
那么在Driver应用内部，包含哪些组件，并如何协作？
在上面的example代码中，看到SparkContext是应用入口，接下来，我们就在SparkContext内部做一个走马观花的黄金周大游行。
SparkContext由一个class和伴生对象组成（这是scala语言的特性，相当于java中类中间吧static method单独抽出来放到一起。）

```
// Create and start the scheduler
private var taskScheduler: TaskScheduler = {
        // Regular expression used for local[N] master format
        val LOCAL_N_REGEX = """local\[([0-9]+)\]""".r
                // Regular expression for local[N, maxRetries], used in tests with failing tasks
                val LOCAL_N_FAILURES_REGEX = """local\[([0-9]+)\s*,\s*([0-9]+)\]""".r
                // Regular expression for simulating a Spark cluster of [N, cores, memory] locally
                val LOCAL_CLUSTER_REGEX = """local-cluster\[\s*([0-9]+)\s*,\s*([0-9]+)\s*,\s*([0-9]+)\s*]""".r
                // Regular expression for connecting to Spark deploy clusters
                val SPARK_REGEX = """(spark://.*)""".r
                //Regular expression for connection to Mesos cluster
                val MESOS_REGEX = """(mesos://.*)""".r

                master match {
                        //...省略...
                        case LOCAL_CLUSTER_REGEX(numSlaves, coresPerSlave, memoryPerSlave) =>
                                // Check to make sure memory requested <= memoryPerSlave. Otherwise Spark will just hang.
                                val memoryPerSlaveInt = memoryPerSlave.toInt
                                if (SparkContext.executorMemoryRequested > memoryPerSlaveInt) {
                                        throw new SparkException(
                                                        "Asked to launch cluster with %d MB RAM / worker but requested %d MB/worker".format(
                                                                memoryPerSlaveInt, SparkContext.executorMemoryRequested))
                                }

                                val scheduler = new ClusterScheduler(this)
                                val localCluster = new LocalSparkCluster(
                                                numSlaves.toInt, coresPerSlave.toInt, memoryPerSlaveInt)
                                val sparkUrl = localCluster.start()
                                val backend = new SparkDeploySchedulerBackend(scheduler, this, sparkUrl, appName)
                                scheduler.initialize(backend)
                                backend.shutdownCallback = (backend: SparkDeploySchedulerBackend) => {
                                        localCluster.stop()
                                }
                                scheduler

                        case "yarn-standalone" =>
                                val scheduler = try {
                                        val clazz = Class.forName("org.apache.spark.scheduler.cluster.YarnClusterScheduler")
                                                val cons = clazz.getConstructor(classOf[SparkContext])
                                                cons.newInstance(this).asInstanceOf[ClusterScheduler]
                                } catch {
                                        // TODO: Enumerate the exact reasons why it can fail
                                        // But irrespective of it, it means we cannot proceed !
                                        case th: Throwable => {
                                                         throw new SparkException("YARN mode not available ?", th)
                                                 }
                                }
                                val backend = new StandaloneSchedulerBackend(scheduler, this.env.actorSystem)
                                scheduler.initialize(backend)
                                scheduler

                                case _ =>
                                if (MESOS_REGEX.findFirstIn(master).isEmpty) {
                                        logWarning("Master %s does not match expected format, parsing as Mesos URL".format(master))
                                }
                                MesosNativeLibrary.load()
                                val scheduler = new ClusterScheduler(this)
                                val coarseGrained = System.getProperty("spark.mesos.coarse", "false").toBoolean
                                val masterWithoutProtocol = master.replaceFirst("^mesos://", "")  // Strip initial mesos://
                                val backend = if (coarseGrained) {
                                        new CoarseMesosSchedulerBackend(scheduler, this, masterWithoutProtocol, appName)
                                } else {
                                        new MesosSchedulerBackend(scheduler, this, masterWithoutProtocol, appName)
                                }
                                scheduler.initialize(backend)
                                scheduler
                }
}
taskScheduler.start()
@volatile private var dagScheduler = new DAGScheduler(taskScheduler)
dagScheduler.start()
ui.start()

```
SparkContext初始化过程当中，按照配置，分别初始化

- TaskScheduler：任务调度
- DAGScheduler：任务生成
- UI：提供监控视图

完成以上工作之后，SparkContext就已经提供了任务管理所需要的全部重要功能。

任务生成
---
在分析任务生成的具体流程之前，需要先了解RDD(Resilient Distributed Dataset)到底抽象了什么东西。最初的paper上面，是这么阐述的：

> We present Resilient Distributed Datasets (RDDs), a dis-
> tributed memory abstraction that lets programmers per-
> form in-memory computations on large clusters in a
> fault-tolerant manner. RDDs are motivated by two types
> of applications that current computing frameworks han-
> dle inefficiently: iterative algorithms and interactive data
> mining tools.
另外一段，

> /**
>  * A Resilient Distributed Dataset (RDD), the basic abstraction in Spark. Represents an immutable,
>  * partitioned collection of elements that can be operated on in parallel. This class contains the
>  * basic operations available on all RDDs, such as `map`, `filter`, and `persist`. In addition,
>  * [[org.apache.spark.rdd.PairRDDFunctions]] contains operations available only on RDDs of key-value
>  * pairs, such as `groupByKey` and `join`; [[org.apache.spark.rdd.DoubleRDDFunctions]] contains
>  * operations available only on RDDs of Doubles; and [[org.apache.spark.rdd.SequenceFileRDDFunctions]]
>  * contains operations available on RDDs that can be saved as SequenceFiles. These operations are
>  * automatically available on any RDD of the right type (e.g. RDD[(Int, Int)] through implicit
>  * conversions when you `import org.apache.spark.SparkContext._`.
>  *
>  * Internally, each RDD is characterized by five main properties:
>  *
>  *  - A list of partitions
>  *  - A function for computing each split
>  *  - A list of dependencies on other RDDs
>  *  - Optionally, a Partitioner for key-value RDDs (e.g. to say that the RDD is hash-partitioned)
>  *  - Optionally, a list of preferred locations to compute each split on (e.g. block locations for
>  *    an HDFS file)
>  *
>  * All of the scheduling and execution in Spark is done based on these methods, allowing each RDD
>  * to implement its own way of computing itself. Indeed, users can implement custom RDDs (e.g. for
>  * reading data from a new storage system) by overriding these functions. Please refer to the
>  * [Spark paper](http://www.cs.berkeley.edu/~matei/papers/2012/nsdi_spark.pdf) for more details
>  * on RDD internals.
>  */

大致翻译过来的意思是说，RDD是为适用于大型的分布式迭代计算和交互数据挖掘构想的in-memory的数据抽象，根据此类计算需求的特征，抽象出RDD五个主要属性。并提供两类operation，分别是transformation和action，输出类型还是RDD的operation定义为transformation，其他的是action。更多RDD的细节，在后面的分析中会进一步设计。这里需要先知道的是，Spark的任务生成，真正的触发是action类型的RDD操作。

回到之前的栗子

```
val count = spark.parallelize(1 to n, slices).map { i =>
        val x = random * 2 - 1
                val y = random * 2 - 1
                if (x*x + y*y < 1) 1 else 0
}.reduce(_ + _)
```

栗子里面，有三瓣肉，分别是

* parallelize
* map
* reduce

其中parallellize和map是transformation，reduce是action。transformation我们暂时不理，重点观察reduce操作

···
/**
 * Reduces the elements of this RDD using the specified commutative and associative binary operator.
 */
def reduce(f: (T, T) => T): T = {
        val cleanF = sc.clean(f)
                val reducePartition: Iterator[T] => Option[T] = iter => {
                        if (iter.hasNext) {
                                Some(iter.reduceLeft(cleanF))
                        } else {
                                None
                        }
                }
        var jobResult: Option[T] = None
                val mergeResult = (index: Int, taskResult: Option[T]) => {
                        if (taskResult != None) {
                                jobResult = jobResult match {
                                        case Some(value) => Some(f(value, taskResult.get))
                                        case None => taskResult
                                }
                        }
                }
        sc.runJob(this, reducePartition, mergeResult)
        // Get the final result out of our Option, or throw an exception if the RDD was empty
        jobResult.getOrElse(throw new UnsupportedOperationException("empty collection"))
}
```

runJob的具体实现

```
/**
 * Run a function on a given set of partitions in an RDD and pass the results to the given
 * handler function. This is the main entry point for all actions in Spark. The allowLocal
 * flag specifies whether the scheduler can run the computation on the driver rather than
 * shipping it out to the cluster, for short actions like first().
 */
def runJob[T, U: ClassManifest](
                rdd: RDD[T],
                func: (TaskContext, Iterator[T]) => U,
                partitions: Seq[Int],
                allowLocal: Boolean,
                resultHandler: (Int, U) => Unit) {
        val callSite = Utils.formatSparkCallSite
                logInfo("Starting job: " + callSite)
                val start = System.nanoTime
                val result = dagScheduler.runJob(rdd, func, partitions, callSite, allowLocal, resultHandler,
                                localProperties.get)
                logInfo("Job finished: " + callSite + ", took " + (System.nanoTime - start) / 1e9 + " s")
                rdd.doCheckpoint()
                result
}
```
从这里开始，进入DAGScheduler内部，正式开始任务生成操作
DAGScheduler中生成任务分成两个部分，第一个部分是封装job的各种属性到一个JobSubmit对象，保存到一个Queue里面，并持有一个JobWaiter对象，等待job执行完毕。另一部是DAGScheduler定时检查Queue，进行任务的实际生成流程。

```
private def run() {
        SparkEnv.set(env)
                while (true) {
                        val event = eventQueue.poll(POLL_TIMEOUT, TimeUnit.MILLISECONDS)
                        if (event != null) {
                                logDebug("Got event of type " + event.getClass.getName)
                        }
                        this.synchronized { // needed in case other threads makes calls into methods of this class
                                if (event != null) {
                                        if (processEvent(event)) {
                                                return
                                        }
                                }

```

接下来，Job的划分流程，有两个阶段

* 生成Stage
* 生成Task

```
case JobSubmitted(finalRDD, func, partitions, allowLocal, callSite, listener, properties) =>
val jobId = nextJobId.getAndIncrement()
val finalStage = newStage(finalRDD, None, jobId, Some(callSite))
val job = new ActiveJob(jobId, finalStage, func, partitions, callSite, listener, properties)
clearCacheLocs()
logInfo("Got job " + job.jobId + " (" + callSite + ") with " + partitions.length +
                " output partitions (allowLocal=" + allowLocal + ")")
logInfo("Final stage: " + finalStage + " (" + finalStage.name + ")")
logInfo("Parents of final stage: " + finalStage.parents)
logInfo("Missing parents: " + getMissingParentStages(finalStage))
if (allowLocal && finalStage.parents.size == 0 && partitions.length == 1) {
        // Compute very short actions like first() or take() with no parent stages locally.
        runLocally(job)
} else {
        listenerBus.post(SparkListenerJobStart(job, properties))
        idToActiveJob(jobId) = job
        activeJobs += job
        resultStageToJob(finalStage) = job
        submitStage(finalStage)
}
```
这里面有个判断，单分区无依赖的job直接就在Driver Program的本地进程执行。这部分很容易，就不展开说了。submitStage开始真正进入分布式计算的节奏。根据RDD的依赖关系，往上准备相关依赖的Stage，未执行的stage暂时保存的waiting存储里面。Stage的依赖处理完成，就开始按照Stage生成Task了。

```
/** Called when stage's parents are available and we can now do its task. */
private def submitMissingTasks(stage: Stage) {
        logDebug("submitMissingTasks(" + stage + ")")
        // Get our pending tasks and remember them in our pendingTasks entry
        val myPending = pendingTasks.getOrElseUpdate(stage, new HashSet)
        myPending.clear()
        var tasks = ArrayBuffer[Task[_]]()
        if (stage.isShuffleMap) {
                for (p <- 0 until stage.numPartitions if stage.outputLocs(p) == Nil) {
                        val locs = getPreferredLocs(stage.rdd, p)
                        tasks += new ShuffleMapTask(stage.id, stage.rdd, stage.shuffleDep.get, p, locs)
                }
        } else {
                // This is a final stage; figure out its job's missing partitions
                val job = resultStageToJob(stage)
                for (id <- 0 until job.numPartitions if !job.finished(id)) {
                        val partition = job.partitions(id)
                        val locs = getPreferredLocs(stage.rdd, partition)
                        tasks += new ResultTask(stage.id, stage.rdd, job.func, partition, locs, id)
                }
        }

        val properties = if (idToActiveJob.contains(stage.jobId)) {
                idToActiveJob(stage.jobId).properties
        } else {
                //this stage will be assigned to "default" pool
                null
        }

        // must be run listener before possible NotSerializableException
        // should be "StageSubmitted" first and then "JobEnded"
        listenerBus.post(SparkListenerStageSubmitted(stage, tasks.size, properties))
        if (tasks.size > 0) {
                // Preemptively serialize a task to make sure it can be serialized. We are catching this
                // exception here because it would be fairly hard to catch the non-serializable exception
                // down the road, where we have several different implementations for local scheduler and
                // cluster schedulers.
                try {
                        SparkEnv.get.closureSerializer.newInstance().serialize(tasks.head)
                } catch {
                        case e: NotSerializableException =>
                                abortStage(stage, e.toString)
                                running -= stage
                                return
                }

                logInfo("Submitting " + tasks.size + " missing tasks from " + stage + " (" + stage.rdd + ")")
                myPending ++= tasks
                logDebug("New pending tasks: " + myPending)
                taskSched.submitTasks(
                                new TaskSet(tasks.toArray, stage.id, stage.newAttemptId(), stage.jobId, properties))
                if (!stage.submissionTime.isDefined) {
                        stage.submissionTime = Some(System.currentTimeMillis())
                }
        } else {
                logDebug("Stage " + stage + " is actually done; %b %d %d".format(
                                stage.isAvailable, stage.numAvailableOutputs, stage.numPartitions))
                running -= stage
        }
}

```
到这里，任务就生成了。任务按照类型分成两类：

- ShuffleMapTask
- ResultTask
这个有个注意点，ResultTask是按照job的partition个数来生成，而ShuffleMapTask是按照stage的partition个数来的，这是应为Shuffle Dependency是宽依赖。

任务调度
---
上面的代码中，Task准备完毕，会由TaskSchedule来提交Task，TaskSchedule又两个实现，LocalSchedule和ClusterSchedule，实现submitTasks函数。后面都以ClusterSchedule为栗子，进行分析，调度逻辑并没有很大的差别。TaskSchedule中以TaskSetManager去维护管理Tasks，并且提供resourceOffer函数响应计算资源对Task的申领。ClusterSchedule以ScheduleBackend负责计算资源的维护（Local的计算资源调度简单，所以就有LocalSchedule本身自行维护），ClusterSchedule以一个抽象的Pool保存任务（实际持有的是TaskSetManager对象），Pool有多个实现，可以调整任务分配的策略，比如FIFO，比如Fair，也可以实现自己的分配策略。任务准备完毕之后，就是想Backend要求计算资源了，ClusterSchedule通过akka Actor与后端通信，实际的请求资源代码如下：


```
// Make fake resource offers on all executors
def makeOffers() {
        launchTasks(scheduler.resourceOffers(
                                executorHost.toArray.map {case (id, host) => new WorkerOffer(id, host, freeCores(id))}))
}

// Launch tasks returned by a set of resource offers
def launchTasks(tasks: Seq[Seq[TaskDescription]]) {
        for (task <- tasks.flatten) {
                freeCores(task.executorId) -= 1
                executorActor(task.executorId) ! LaunchTask(task)
        }
}
```
这段代码值得仔细思考，首先向拿所有空闲资源向Schedule要求任务，说我又这么多空闲资源，你分配给我合适的任务。然后launchTasks负责把各任务通知到实际的计算执行者。这模块会在后面运算执行部分详细说明。

```
/**
 * Called by cluster manager to offer resources on slaves. We respond by asking our active task
 * sets for tasks in order of priority. We fill each node with tasks in a round-robin manner so
 * that tasks are balanced across the cluster.
 */
def resourceOffers(offers: Seq[WorkerOffer]): Seq[Seq[TaskDescription]] = synchronized {
        SparkEnv.set(sc.env)

        // Mark each slave as alive and remember its hostname
        for (o <- offers) {
                executorIdToHost(o.executorId) = o.host
                if (!executorsByHost.contains(o.host)) {
                        executorsByHost(o.host) = new HashSet[String]()
                                executorGained(o.executorId, o.host)
                }
        }

        // Build a list of tasks to assign to each worker
        val tasks = offers.map(o => new ArrayBuffer[TaskDescription](o.cores))
        val availableCpus = offers.map(o => o.cores).toArray
        val sortedTaskSets = rootPool.getSortedTaskSetQueue()
        for (taskSet <- sortedTaskSets) {
                logDebug("parentName: %s, name: %s, runningTasks: %s".format(
                                        taskSet.parent.name, taskSet.name, taskSet.runningTasks))
        }

        // Take each TaskSet in our scheduling order, and then offer it each node in increasing order
        // of locality levels so that it gets a chance to launch local tasks on all of them.
        var launchedTask = false
        for (taskSet <- sortedTaskSets; maxLocality <- TaskLocality.values) {
                do {
                        launchedTask = false
                                for (i <- 0 until offers.size) {
                                        val execId = offers(i).executorId
                                        val host = offers(i).host
                                        for (task <- taskSet.resourceOffer(execId, host, availableCpus(i), maxLocality)) {
                                                tasks(i) += task
                                                val tid = task.taskId
                                                taskIdToTaskSetId(tid) = taskSet.taskSet.id
                                                taskSetTaskIds(taskSet.taskSet.id) += tid
                                                taskIdToExecutorId(tid) = execId
                                                activeExecutorIds += execId
                                                executorsByHost(host) += execId
                                                availableCpus(i) -= 1
                                                launchedTask = true
                                        }
                                }
                } while (launchedTask)
        }

        if (tasks.size > 0) {
                hasLaunchedTask = true
        }
        return tasks
}
```
遍历，然后由每一个TaskSetManager负责实际提供Task

```
/**
 * Respond to an offer of a single executor from the scheduler by finding a task
 */
override def resourceOffer(
                execId: String,
                host: String,
                availableCpus: Int,
                maxLocality: TaskLocality.TaskLocality)
        : Option[TaskDescription] =
{
        if (tasksFinished < numTasks && availableCpus >= CPUS_PER_TASK) {
                val curTime = clock.getTime()

                        var allowedLocality = getAllowedLocalityLevel(curTime)
                        if (allowedLocality > maxLocality) {
                                allowedLocality = maxLocality   // We're not allowed to search for farther-away tasks
                        }

                findTask(execId, host, allowedLocality) match {
                        case Some((index, taskLocality)) => {
                                // Found a task; do some bookkeeping and return a task description
                                val task = tasks(index)
                                        val taskId = sched.newTaskId()
                                        // Figure out whether this should count as a preferred launch
                                        logInfo("Starting task %s:%d as TID %s on executor %s: %s (%s)".format(
                                                                taskSet.id, index, taskId, execId, host, taskLocality))
                                        // Do various bookkeeping
                                        copiesRunning(index) += 1
                                        val info = new TaskInfo(taskId, index, curTime, execId, host, taskLocality)
                                        taskInfos(taskId) = info
                                        taskAttempts(index) = info :: taskAttempts(index)
                                        // Update our locality level for delay scheduling
                                        currentLocalityIndex = getLocalityIndex(taskLocality)
                                        lastLaunchTime = curTime
                                        // Serialize and return the task
                                        val startTime = clock.getTime()
                                        // We rely on the DAGScheduler to catch non-serializable closures and RDDs, so in here
                                        // we assume the task can be serialized without exceptions.
                                        val serializedTask = Task.serializeWithDependencies(
                                                        task, sched.sc.addedFiles, sched.sc.addedJars, ser)
                                        val timeTaken = clock.getTime() - startTime
                                        increaseRunningTasks(1)
                                        logInfo("Serialized task %s:%d as %d bytes in %d ms".format(
                                                                taskSet.id, index, serializedTask.limit, timeTaken))
                                        val taskName = "task %s:%d".format(taskSet.id, index)
                                        if (taskAttempts(index).size == 1)
                                                taskStarted(task,info)
                                                        return Some(new TaskDescription(taskId, execId, taskName, index, serializedTask))
                        }
                        case _ =>
                }
        }
        return None
}
```
如此，任务管理的全部过程就梳理清楚了，以上过程，都是在一个单独的虚拟机进程（Driver Program）中执行的，到现在为止，还没有涉及到分布式什么的，感觉好奇怪。哈哈哈，休息一下，待会儿继续运算执行部分，这部分就是。

计算执行
===
